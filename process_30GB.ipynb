{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060d3e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                     int64\n",
      "timestamp             object\n",
      "user_name             object\n",
      "chinese_name          object\n",
      "email                 object\n",
      "age                    int64\n",
      "income               float64\n",
      "gender                object\n",
      "country               object\n",
      "chinese_address       object\n",
      "purchase_history      object\n",
      "is_active               bool\n",
      "registration_date     object\n",
      "credit_score           int64\n",
      "phone_number          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "parquet_file = pq.ParquetFile('C:\\\\Users\\\\admin\\\\Downloads\\\\30G_data\\\\part-00000.parquet')\n",
    "data = parquet_file.read().to_pandas()\n",
    "\n",
    "# data.iloc[0]\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b1d741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18750000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_active = data[data['is_active']==False]\n",
    "len(is_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e6801b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a5e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import pyarrow\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体\n",
    "# 启用进度条\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a993d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "开始读取数据集: 10GB_data\n",
      "========================================\n",
      "开始读取文件: C:/Users/admin/Downloads/10G_data/*.parquet\n",
      "发现 8 个数据文件\n",
      "数据读取完成，耗时：0.01秒\n",
      "数据读取耗时: 0.01秒\n",
      "[########################################] | 100% Completed | 107.91 ms\n",
      "数据集大小: 100000000 行, 15 列\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "def read_multiple_parquet(pattern):\n",
    "    \"\"\"读取并优化数据格式\"\"\"\n",
    "    print(f\"开始读取文件: {pattern}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"发现 {len(files)} 个数据文件\")\n",
    "    \n",
    "    # 列类型映射（优化内存使用）\n",
    "    type_map = {\n",
    "        'id': 'category',\n",
    "        'user_name': 'category',\n",
    "        'chinese_name': 'category',\n",
    "        'email': 'category',\n",
    "        'gender': 'category',\n",
    "        'country': 'category',\n",
    "        'is_active': 'bool',\n",
    "        'timestamp': 'datetime64[ns]',\n",
    "        'registration_date': 'datetime64[ns]',\n",
    "        'age': 'int8',\n",
    "        'income': 'float32',\n",
    "        'credit_score': 'int16'\n",
    "    }\n",
    "    \n",
    "    ddf = dd.read_parquet(\n",
    "        files,\n",
    "        engine='pyarrow',\n",
    "        dtype=type_map,\n",
    "        parse_dates=['timestamp', 'registration_date'],\n",
    "        # blocksize=\"256MB\"  # 控制每个分区的大小\n",
    "    )\n",
    "    ddf['timestamp'] = dd.to_datetime(ddf['timestamp'], format='%Y-%m-%dT%H:%M:%S%z')\n",
    "    ddf['registration_date'] = dd.to_datetime(ddf['registration_date'], format='%Y-%m-%d')\n",
    "    # print(ddf.dtypes)\n",
    "    # 处理中文地址的特殊字符\n",
    "    ddf['chinese_address'] = ddf['chinese_address'].astype('string')\n",
    "    \n",
    "    print(f\"数据读取完成，耗时：{time.time()-start:.2f}秒\")\n",
    "    return ddf\n",
    "\n",
    "datasets = {\n",
    "    \"30GB_data\": \"C:/Users/admin/Downloads/30G_data/*.parquet\"\n",
    "}\n",
    "\n",
    "for ds_name, pattern in datasets.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"开始读取数据集: {ds_name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    ds_report = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    read_start = time.time()\n",
    "    ddf = read_multiple_parquet(pattern)\n",
    "    ds_report['read_time'] = time.time() - read_start\n",
    "    print(f\"数据读取耗时: {ds_report['read_time']:.2f}秒\")\n",
    "    print(f\"数据集大小: {ddf.shape[0].compute()} 行, {ddf.shape[1]} 列\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44fe9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[数值型字段统计]\n",
      "[########################################] | 100% Completed | 4.76 ss\n",
      "                age        income  credit_score\n",
      "count  1.000000e+08  1.000000e+08  1.000000e+08\n",
      "mean   5.901673e+01  4.993547e+05  5.751113e+02\n",
      "std    2.395049e+01  2.890191e+05  1.590030e+02\n",
      "min    1.800000e+01  0.000000e+00  3.000000e+02\n",
      "25%    3.800000e+01  2.520000e+05  4.380000e+02\n",
      "50%    5.900000e+01  5.020000e+05  5.760000e+02\n",
      "75%    8.000000e+01  7.520000e+05  7.140000e+02\n",
      "max    1.000000e+02  1.000000e+06  8.500000e+02\n",
      "[########################################] | 100% Completed | 159.42 s\n",
      "[########################################] | 100% Completed | 110.02 ms\n",
      "[                                        ] | 0% Completed | 278.10 us"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_716\\3195666943.py:24: UserWarning: Glyph 8722 (\\N{MINUS SIGN}) missing from font(s) SimHei.\n",
      "  plt.savefig(f'{dataset_name}_missing_values.png', bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 1.04 sms\n",
      "[########################################] | 100% Completed | 32.56 s\n",
      "探索性分析完成，耗时：202.67秒\n"
     ]
    }
   ],
   "source": [
    "def enhanced_analysis(ddf, dataset_name):\n",
    "    \"\"\"执行数据集探索性分析\"\"\"\n",
    "    start = time.time()\n",
    "    results = {}\n",
    "    \n",
    "    # 基础统计\n",
    "    print(\"\\n[数值型字段统计]\")\n",
    "    num_stats = ddf[['age', 'income', 'credit_score']].describe().compute()\n",
    "    print(num_stats)\n",
    "    results['numeric_stats'] = num_stats.to_dict()\n",
    "    \n",
    "    # 缺失值分析\n",
    "    missing = ddf.isna().sum().compute()\n",
    "    total = len(ddf)\n",
    "    missing_pct = (missing / total * 100).round(2)\n",
    "    missing_df = pd.DataFrame({'缺失数量': missing, '缺失比例(%)': missing_pct})\n",
    "    \n",
    "    # 可视化缺失值\n",
    "    plt.figure(figsize=(12,6))\n",
    "    missing_df['缺失比例(%)'].sort_values().plot(kind='barh', color='skyblue')\n",
    "    plt.title(f'{dataset_name} - 字段缺失值分布',fontfamily='SimHei')\n",
    "    plt.xlabel('缺失比例(%)',fontfamily='SimHei')\n",
    "    plt.ylabel('字段名称',fontfamily='SimHei')\n",
    "    plt.savefig(f'{dataset_name}_missing_values.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 用户年龄分布\n",
    "    plt.figure(figsize=(10,6))\n",
    "    ddf['age'].compute().plot(kind='hist', bins=50, alpha=0.7)\n",
    "    plt.title(f'{dataset_name} - 用户年龄分布',fontfamily='SimHei')\n",
    "    plt.xlabel('年龄', fontfamily='SimHei')\n",
    "    plt.ylabel('用户数量', fontfamily='SimHei')\n",
    "    plt.savefig(f'{dataset_name}_age_dist.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 用户活跃度分析\n",
    "    if 'registration_date' in ddf.columns:\n",
    "        ddf['reg_year'] = ddf['registration_date'].dt.year\n",
    "        reg_dist = ddf['reg_year'].value_counts().compute().sort_index()\n",
    "        \n",
    "        plt.figure(figsize=(10,6))\n",
    "        reg_dist.plot(kind='bar', color='teal')\n",
    "        plt.title(f'{dataset_name} - 用户注册年份分布',fontfamily='SimHei')\n",
    "        plt.xlabel('注册年份', fontfamily='SimHei')\n",
    "        plt.ylabel('用户数量', fontfamily='SimHei')\n",
    "        plt.savefig(f'{dataset_name}_reg_year.png')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"探索性分析完成，耗时：{time.time()-start:.2f}秒\")\n",
    "    return results\n",
    "    \n",
    "for ds_name, pattern in datasets.items():\n",
    "    analysis_results = enhanced_analysis(ddf, ds_name)\n",
    "    ds_report.update(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d2768bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   False\n",
       "timestamp            False\n",
       "user_name            False\n",
       "chinese_name         False\n",
       "email                False\n",
       "age                  False\n",
       "income               False\n",
       "gender               False\n",
       "country              False\n",
       "chinese_address      False\n",
       "purchase_history     False\n",
       "is_active            False\n",
       "registration_date    False\n",
       "credit_score         False\n",
       "phone_number         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe137b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 107.08 ms\n",
      "[########################################] | 100% Completed | 162.83 s\n",
      "[########################################] | 100% Completed | 1.95 ss\n",
      "[########################################] | 100% Completed | 157.96 s\n",
      "数据预处理完成，耗时：323.01秒\n",
      "数据量变化：100000000 -> 100000000\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "def enhanced_preprocessing(ddf, dataset_name):\n",
    "    \"\"\"数据清洗流程\"\"\"\n",
    "    start = time.time()\n",
    "    original_count = len(ddf)\n",
    "    results = {}\n",
    "    \n",
    "    # 处理缺失值\n",
    "    missing = ddf.isna().sum().compute()\n",
    "    missing_cols = missing[missing > 0].index.tolist()\n",
    "    \n",
    "    # 删除高缺失率字段（>40%）\n",
    "    high_missing = missing[(missing/original_count) > 0.4].index\n",
    "    ddf = ddf.drop(columns=high_missing)\n",
    "    results['dropped_columns'] = high_missing.tolist()\n",
    "    \n",
    "    # 处理年龄异常值\n",
    "    age_filter = (ddf['age'] >= 18) & (ddf['age'] <= 100)\n",
    "    ddf = ddf[age_filter]\n",
    "    \n",
    "    # 处理收入异常值（IQR方法）\n",
    "    if 'income' in ddf.columns:\n",
    "        q = ddf['income'].quantile([0.25, 0.75]).compute()\n",
    "        iqr = q[0.75] - q[0.25]\n",
    "        income_filter = (ddf['income'] >= (q[0.25] - 1.5*iqr)) & (ddf['income'] <= (q[0.75] + 1.5*iqr))\n",
    "        ddf = ddf[income_filter]\n",
    "    \n",
    "    # 处理信用评分\n",
    "    if 'credit_score' in ddf.columns:\n",
    "        ddf = ddf[(ddf['credit_score'] >= 300) & (ddf['credit_score'] <= 850)]\n",
    "    \n",
    "    # 保存预处理结果\n",
    "    processed_count = len(ddf)\n",
    "    results['original_count'] = original_count\n",
    "    results['processed_count'] = processed_count\n",
    "    results['processing_time'] = time.time() - start\n",
    "    \n",
    "    print(f\"数据预处理完成，耗时：{results['processing_time']:.2f}秒\")\n",
    "    print(f\"数据量变化：{original_count} -> {processed_count}\")\n",
    "    \n",
    "    return ddf, results\n",
    "\n",
    "for ds_name, pattern in datasets.items():\n",
    "    preprocess_start = time.time()\n",
    "    cleaned_ddf, preprocess_results = enhanced_preprocessing(ddf, ds_name)\n",
    "    ds_report.update(preprocess_results)\n",
    "    ds_report['preprocess_time'] = time.time() - preprocess_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b713d306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 154.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "isactive = ddf[ddf['is_active']==False]\n",
    "len(isactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba4d81c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 154.77 s\n",
      "发现 6840250 个潜在高价值用户:\n",
      "Dask DataFrame Structure:\n",
      "                     id       timestamp user_name chinese_name   email      age   income  gender country chinese_address purchase_history is_active registration_date credit_score phone_number reg_year\n",
      "npartitions=96                                                                                                                                                                                          \n",
      "                float64  datetime64[ns]    string       string  string  float64  float64  string  string          string           string    object    datetime64[ns]      float64       string    int32\n",
      "                    ...             ...       ...          ...     ...      ...      ...     ...     ...             ...              ...       ...               ...          ...          ...      ...\n",
      "...                 ...             ...       ...          ...     ...      ...      ...     ...     ...             ...              ...       ...               ...          ...          ...      ...\n",
      "                    ...             ...       ...          ...     ...      ...      ...     ...     ...             ...              ...       ...               ...          ...          ...      ...\n",
      "                    ...             ...       ...          ...     ...      ...      ...     ...     ...             ...              ...       ...               ...          ...          ...      ...\n",
      "Dask Name: getitem, 19 expressions\n",
      "Expr=Filter(frame=Assign(frame=Assign(frame=Assign(frame=Assign(frame=ReadParquetFSSpec(9ea8a3b))))), predicate=Assign(frame=Assign(frame=Assign(frame=Assign(frame=ReadParquetFSSpec(9ea8a3b)))))['income'] >= 750000 & Assign(frame=Assign(frame=Assign(frame=Assign(frame=ReadParquetFSSpec(9ea8a3b)))))['credit_score'] >= 700)\n",
      "高价值用户分析耗时: 154.83秒\n"
     ]
    }
   ],
   "source": [
    "# 识别潜在高价值用户\n",
    "def high_value_user_analysis(ddf):\n",
    "    \"\"\"识别高价值用户\"\"\"\n",
    "    start = time.time()\n",
    "    high_value_users = ddf[\n",
    "        (ddf['income'] >= 750000) & \n",
    "        (ddf['credit_score'] >= 700)\n",
    "    ]\n",
    "\n",
    "    # 展示结果（可选）\n",
    "    print(f\"发现 {len(high_value_users)} 个潜在高价值用户:\")\n",
    "    print(high_value_users)\n",
    "    process_time = time.time() - start\n",
    "    print(f\"高价值用户分析耗时: {process_time:.2f}秒\")\n",
    "    return high_value_users\n",
    "\n",
    "for ds_name, pattern in datasets.items():\n",
    "    preprocess_start = time.time()\n",
    "    high_value_users = high_value_user_analysis(ddf)\n",
    "    ds_report['preprocess_time'] = time.time() - preprocess_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fc961",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
